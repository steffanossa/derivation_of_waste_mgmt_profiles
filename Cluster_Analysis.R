# 3 Cluster Analysis -----------------------------------------------------------
# Nutzen Sie Ihre neuen Dimensionen als Basis, um mit Hilfe einer Clusteranalyse
# Profile der Gemeinden herzuleiten. Sie sollten nicht mehr als zehn Profile
# herausarbeiten. Kennzeichnen Sie ihre endgültige Lösung.
#------------------------------------------------------------------------------'
## Hopkins statistic =========================================================== 
#' Hopkins statistic is used to assess the clustering tendency of a data set by
#' measuring the probability that a given data set is generated by a uniform
#' data distribution.
#' A value above .75 indicates a clustering tendency at 90% conf level
#' https://www.datanovia.com/en/lessons/assessing-clustering-tendency/
wm_df_transformed_pca %>% get_clust_tendency(n = nrow(wm_df_transformed_pca) - 1,
                                             graph = F)
#' .7793498. Quite good actually.

## Amount of Clusters ==========================================================
wm_df_transformed_pca %>%  fviz_nbclust(hcut, method = wss)


## Hierarchical Clustering =====================================================
### Agglomerative Methods ######################################################
#' Starting with a single observation, successively adding more.
#' There is are numbers of different methods for both, creating a distance matrix
#' and creating the clusters.
dist_meth <- c("euclidean", "maximum", "manhattan", "canberra", "minkowski")
clust_meth <- c("single", "complete", "average", "mcquitty", "median", "centroid", "ward.D2")
#' Plots into .pdf
pdf("hclust_dendro_combinations.pdf", onefile = TRUE)
#' For loop iterating over each dist_meth
for (dm in dist_meth) {
  #' For-loop iterating over each clust_meth
  for (cm in clust_meth) {
    hclustt <- 
      dist(wm_df_transformed_pca, method = dm) %>% 
      hclust(method = cm)
    ggdendrogram(hclustt, leaf_labels = F, labels = F) +
      labs(title = paste0("Distance: ", dm, "\nCluster: ", cm))
    #dendro_plot_list <- apply(dendro_plot_list, p)
  }
}
dev.off()
#' *DOES NOT WORK, IDK WHY*

#' Manhattan + ward.D2 looks best.
hclustt <- 
  dist(scale(wm_df_transformed_pca), 
       method = "canberra") %>% 
  hclust(method = "ward.D2")
ggdendrogram(hclustt, leaf_labels = F, labels = F) +
  labs(title = paste0("Distance: ", "\nCluster: "))

hclust_a <- dist(scale(wm_df_transformed_pca,
                       center = TRUE,
                       scale = TRUE),
                 method = "manhattan") %>% 
  hclust(method = "ward.D2")
clust_a <- cutree(hclust_a, k = 4)

#' Bind cluster id to data set
wm_df_prepped_clust_a <- wm_df_prepped %>% mutate(Cluster_a = clust_a)

#### Profiling using Microsoft Excel ###########################################
for (i in 2:6) {
  wm_df_prepped_clust_a <-
    wm_df_prepped %>%
    mutate(Cluster_a = cutree(hclust_a, k = i))
  
  profiling_df <-
    wm_df_prepped_clust_a %>% 
    group_by(Cluster_a) %>% 
    summarize_all(mean)
  profiling_df$n_Cluster <- i
  profiling_df$Size <- table(wm_df_prepped_clust_a$Cluster_a)
  
  if (i == 2) {
    profiling_df_final <- profiling_df
  } else {
    profiling_df_final <-
      profiling_df_final %>% 
      rows_append(profiling_df)
  }
}

write.csv2(profiling_df_final, file = "profiling_clust_a.csv")

### Divisive Methods ###########################################################
#' Starting with a single cluster containing all observations, splitting up more
#' and more.
#### Diana (DIvisive, ANAlysis Clustering) #####################################
##### Daisy (Dissimilarity Matrix Calculation) #################################
dist_meth_daisy <- c("euclidean", "manhattan", "gower")
clust_meth_agnes <- c("single", "complete", "weighted", "ward", "flexible")
##### Agnes (AGlomerative NESting) #############################################
#### Mona (MONothetic Analysis) ################################################
## Partiotional Clustering =====================================================
### K-Means ####################################################################
#' Optimal number of clusters using withinSS
fviz_nbclust(wm_df_transformed_pca, kmeans, method = "wss")
#' 3
fviz_nbclust(wm_df_transformed_pca, kmeans, method = "silhouette")
#' 3
fviz_nbclust(wm_df_transformed_pca, kmeans, method = "gap_stat")
#' 9. No.
kmeans(wm_df_transformed_pca, centers = 3, iter.max = 10, nstart = 10)

#### Profiling using Microsoft Excel ###########################################
for (i in 2:5) {
  kmeans_t <- kmeans(scale(wm_df_transformed_pca),
                   centers = i,
                   nstart = 10,
                   iter.max = 10
                   )
  
  wm_df_prepped_clust_kmeans <-
    wm_df_prepped %>% 
    mutate(Cluster_k = kmeans_t$cluster)
  
  profiling_df_kmeans <-
    wm_df_prepped_clust_kmeans %>% 
    group_by(Cluster_k) %>% 
    summarise_all(mean)
  
  profiling_df_kmeans$n_Cluster <- i
  profiling_df_kmeans$Size <- table(wm_df_prepped_clust_kmeans$Cluster_k)
  
  if (i == 2) {
    profiling_df_kmeans_final <- profiling_df_kmeans
  } else {
    profiling_df_kmeans_final <-
      profiling_df_kmeans_final %>% 
      rows_append(profilings_df)
  }
}

write.csv2(profiling_df_kmeans_final, file = "profiling_kmeans.csv")

#' Decided to go with hclust after inspection in Excel
